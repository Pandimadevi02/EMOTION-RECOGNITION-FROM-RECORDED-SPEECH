
About
Emotion is a multifaceted and personal mental state marked by feelings, encompassing both physiological and cognitive aspects.Emotion Recognition from Recorded speech, a captivating yet demanding aspect of human-computer interaction, involves identifying emotional states conveyed through speech by analyzing tone and pitch variations. Emotion recognition from recorded speech, exploring methodologies and techniques to effectively categorize human emotions using machine learning models. The study focuses on (RAVDESS) dataset, employing diverse classifiers such as decision trees, support vector machines, multilayer perceptron’s, and convolutional neural networks. Project discusses the performance and limitations of the emotion recognition system, shedding light on its practical applications across various domains. In this project, we consolidated datasets containing approximately seven main emotions: Happy, Fear, Angry, Disgust, Surprised, Sad, or Neutral, into a single file, then utilized this combined dataset to train the model. After preprocessing the input audio files, including noise reduction using MFCC, structured the sequential data into a 3D array compatible with the CNN model. Visualization of the processed data was facilitated using the Matplotlib library. Through iterative testing with different parameter values, model achieved an average accuracy of 71% during testing and 96% during the training phase.


Features
Accuracy, Speed and Low Latency
Suicde preventation
E-LEARNING PLATFORM




Requirements
Operating System: Requires a 64-bit OS (Windows 10 or Ubuntu) for compatibility with deep learning frameworks.
Development Environment: Python 3.6 or later is necessary for coding the sign language detection system.
Deep Learning Frameworks: TensorFlow for model training, MediaPipe for hand gesture recognition.
Version Control: Implementation of Git for collaborative development and effective code management.
IDE: Use of VSCode as the Integrated Development Environment for coding, debugging, and version control integration.
Additional Dependencies: Includes scikit-learn, TensorFlow (versions 2.4.1), TensorFlow GPU, OpenCV, and Mediapipe for deep learning tasks.

System Architecture
![Uploading image.png…]()



Output
Output1 - Name of the output


Output2 - Name of the output



Detection Accuracy: 85% Note: These metrics can be customized based on your actual performance evaluations.

Results and Impact
This project successfully built a machine learning model to classify emotion from recorded  recordings. 
The model achieved an accuracy of 85% on the validation set and was able to identify emotion in new audio recordings with high confidence.
The comparison of different machine learning models provides valuable insights into their effectiveness in emotion classification tasks.
Overall, project highlights the potential of emotion recognition systems to enhance communication, education, automotive safety, security, and customer service



Articles published
Wang, C., Ren, Y., Zhang, N., Cui, F., & Luo, S. (2022). Speech emotion recognition based on multi feature and multi lingual fusion. Multimedia Tools and Applications, 

Atmaja, B.T.; Sasou, A. Sentiment Analysis and Emotion Recognition from Speech Using Universal Speech Representations. Sensors 2022, 

